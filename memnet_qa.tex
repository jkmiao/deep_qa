\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

%TODO(pradeep): Tentative title pending results.
\title{Memory Networks for Answering Science Questions}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Memory Networks have previously been used for language understanding tasks that require inference over long-term memory. In this work, we use them to answer science questions, which require more complex reasoning than the QA datasets previously used to test memory networks. Directly using end-to-end memory networks \cite{sukhbaatar2015end} gives poor results, showing the difficulty of the problem. We propose ways to improve the performance by pretraining the entailment module and the attention component in the memory network separately.
\end{abstract}
\section{Introduction}

\section{Approach}
\subsection{Memory Network Setup}
\subsection{Pre-training for Entailment}
\subsection{Pre-training Memory Selection}


\section{Experiments}
\subsection{Datasets}
\subsection{Results}
\subsection{Discussion}

\section{Related Work}


\section{Conclusion}
\bibliography{memnet_qa}
\bibliographystyle{eacl2017}
\end{document}